---
title: "model_fitting_part_2"
author: "Joe Martin"
date: "10/18/2021"
output: pdf_document
---

```{r load}
pacman::p_load(pacman, tidyverse, tidymodels, here, readr, broom.mixed, skimr, rsample)

data <- here::here("data","processed_data","processeddata.rds")
df <- read_rds(data)

# Write code that takes the data and splits it randomly into a train and test that, following for instance the example in the Data Splitting section of the Get Started tidymodels tutorial.

df$high_temp <- ifelse(df$BodyTemp > 99, "1","0")
df$high_temp <- factor(high_temp)

# Use 70/30 split on data

data_split <- initial_split(df, prop = 7/10)

train_data <- training(data_split)
test_data <- testing(data_split)
```

```{r}
# Next, following the example in the Create Recipes section of the Get Started tidymodels tutorial, create a simple recipe that fits our categorical outcome of interest to all predictors (we’ll start with categorical and all predictors since that’s the closest to the shown example). For now, you can ignore the concept of roles and features they mention.

sneeze_rec <- recipe(high_temp ~ ., data = train_data)
sneeze_test <- recipe(high_temp ~ ., data = test_data)
```

```{r}
#Set a model as you did in the previous exercise, then use the workflow() package to create a simple workflow that fits a logistic model to all predictors using the glm function. To that end, follow the Fit a model with a recipe section of the tutorial and adjust for your case.

#You should end up with a fit object similar to the one shown at the end of that section in the tutorial - of course, yours will look somewhat different since you are using a different dataset, but overall things should look similar.

summary(sneeze_rec)

lr_model <- logistic_reg() %>%
  set_engine("glm")

sneeze_workflow <- 
  workflow() %>%
  add_model(lr_model) %>%
  add_recipe(sneeze_rec)

sneeze_workflow
```

```{r}
sneeze_fit <- sneeze_workflow %>%
  fit(data = train_data)
```

```{r}
#Follow the example in the Use a trained workflow to predict section of the tutorial to look at the predictions, ROC and ROC-AUC for your data. Apply it to both the training and the test data. ROC and ROC-AUC is another common performance measure/metric for categorical outcomes. If you are not familiar with it, you can read more about them by following the link in the tutorial. It’s not too important to go into the details for now. The focus here is on getting the code to work. In general, a ROC-AUC of 0.5 means the model is no good, 1 is a perfect model. Generally, somewhere above 0.7 do people think the model might be useful.
```

```{r}
#Let’s re-do the fitting but now with a model that only fits the main predictor to the categorical outcome. You should notice that the only thing you have to change is to set up a new recipe, this time one that only has the name of the predictor of interest on the right side of the formula (instead of the . symbol, which is shorthand notation for “all predictors”.) Then you can set up a new workflow with the new recipe, rerun the fit and evaluate performance using the same code as above. In general, if you do multiple models/recipes, you might want to write a loop to go over them, or parallelize/vectorize things. For now, just copying and pasting most of the code is ok.
```
